{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOqR_NmnA72D",
        "outputId": "b66b5bb4-21c7-4aa3-9fa4-15d1fbb392d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Lab5_Benyahia_Mohamed/code/data/')"
      ],
      "metadata": {
        "id": "vGfp2Al-i931"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/drive/MyDrive/Lab5_Benyahia_Mohamed/code/part1/visualization.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT3T8pFfvcUM",
        "outputId": "5c55fe09-014a-4f1b-e603-2f8f74435bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 33226\n",
            "Number of edges: 354529\n",
            "Generating walks\n",
            "Training word2vec\n",
            "Figure(2000x1500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/drive/MyDrive/Lab5_Benyahia_Mohamed/code/part1/node_classification.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6YnmLLtDptz",
        "outputId": "bdf3e09a-0867-474a-ab30-6290ce090604"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 34\n",
            "Number of edges: 78\n",
            "Figure(640x480)\n",
            "Generating walks\n",
            "Training word2vec\n",
            "DeepWalk Embedding Accuracy Score : 1.0\n",
            "Spectral Embedding Accuracy Score : 0.8571428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/drive/MyDrive/Lab5_Benyahia_Mohamed/code/part2/gnn_karate.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksxrh4a2iyLq",
        "outputId": "9d78fa9f-0f2b-4eb1-ee78-5a794238d1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "78\n",
            "/content/drive/MyDrive/ALTEGRAD_lab_5_DLForGraphs_2024/code/part2/utils.py:55: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "Epoch: 001 loss_train: 0.6881 acc_train: 0.5556 time: 1.4187s\n",
            "Epoch: 002 loss_train: 0.6849 acc_train: 0.5556 time: 0.0055s\n",
            "Epoch: 003 loss_train: 0.6827 acc_train: 0.5556 time: 0.0050s\n",
            "Epoch: 004 loss_train: 0.6798 acc_train: 0.5556 time: 0.0042s\n",
            "Epoch: 005 loss_train: 0.6771 acc_train: 0.5556 time: 0.0041s\n",
            "Epoch: 006 loss_train: 0.6739 acc_train: 0.5556 time: 0.0066s\n",
            "Epoch: 007 loss_train: 0.6660 acc_train: 0.5556 time: 0.0041s\n",
            "Epoch: 008 loss_train: 0.6608 acc_train: 0.5556 time: 0.0042s\n",
            "Epoch: 009 loss_train: 0.6503 acc_train: 0.5556 time: 0.0042s\n",
            "Epoch: 010 loss_train: 0.6392 acc_train: 0.5556 time: 0.0041s\n",
            "Epoch: 011 loss_train: 0.6272 acc_train: 0.5926 time: 0.0040s\n",
            "Epoch: 012 loss_train: 0.6086 acc_train: 0.6296 time: 0.0045s\n",
            "Epoch: 013 loss_train: 0.5900 acc_train: 0.7778 time: 0.0039s\n",
            "Epoch: 014 loss_train: 0.5826 acc_train: 0.7778 time: 0.0045s\n",
            "Epoch: 015 loss_train: 0.5489 acc_train: 0.8889 time: 0.0046s\n",
            "Epoch: 016 loss_train: 0.5277 acc_train: 0.9259 time: 0.0047s\n",
            "Epoch: 017 loss_train: 0.5008 acc_train: 0.9630 time: 0.0050s\n",
            "Epoch: 018 loss_train: 0.4661 acc_train: 0.9630 time: 0.0047s\n",
            "Epoch: 019 loss_train: 0.4399 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 020 loss_train: 0.4023 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 021 loss_train: 0.3817 acc_train: 0.9630 time: 0.0048s\n",
            "Epoch: 022 loss_train: 0.3458 acc_train: 1.0000 time: 0.0065s\n",
            "Epoch: 023 loss_train: 0.3059 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 024 loss_train: 0.3016 acc_train: 0.9630 time: 0.0047s\n",
            "Epoch: 025 loss_train: 0.2527 acc_train: 0.9630 time: 0.0048s\n",
            "Epoch: 026 loss_train: 0.2356 acc_train: 0.9630 time: 0.0047s\n",
            "Epoch: 027 loss_train: 0.1831 acc_train: 0.9630 time: 0.0064s\n",
            "Epoch: 028 loss_train: 0.1520 acc_train: 1.0000 time: 0.0048s\n",
            "Epoch: 029 loss_train: 0.1319 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 030 loss_train: 0.1080 acc_train: 1.0000 time: 0.0046s\n",
            "Epoch: 031 loss_train: 0.0864 acc_train: 1.0000 time: 0.0068s\n",
            "Epoch: 032 loss_train: 0.0993 acc_train: 1.0000 time: 0.0048s\n",
            "Epoch: 033 loss_train: 0.0799 acc_train: 1.0000 time: 0.0048s\n",
            "Epoch: 034 loss_train: 0.0632 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 035 loss_train: 0.0629 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 036 loss_train: 0.0539 acc_train: 1.0000 time: 0.0042s\n",
            "Epoch: 037 loss_train: 0.0433 acc_train: 1.0000 time: 0.0043s\n",
            "Epoch: 038 loss_train: 0.0323 acc_train: 1.0000 time: 0.0041s\n",
            "Epoch: 039 loss_train: 0.0296 acc_train: 1.0000 time: 0.0043s\n",
            "Epoch: 040 loss_train: 0.0235 acc_train: 1.0000 time: 0.0056s\n",
            "Epoch: 041 loss_train: 0.0473 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 042 loss_train: 0.0254 acc_train: 1.0000 time: 0.0041s\n",
            "Epoch: 043 loss_train: 0.0295 acc_train: 1.0000 time: 0.0042s\n",
            "Epoch: 044 loss_train: 0.0124 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 045 loss_train: 0.0153 acc_train: 1.0000 time: 0.0046s\n",
            "Epoch: 046 loss_train: 0.0093 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 047 loss_train: 0.0111 acc_train: 1.0000 time: 0.0050s\n",
            "Epoch: 048 loss_train: 0.0096 acc_train: 1.0000 time: 0.0042s\n",
            "Epoch: 049 loss_train: 0.0172 acc_train: 1.0000 time: 0.0040s\n",
            "Epoch: 050 loss_train: 0.0115 acc_train: 1.0000 time: 0.0048s\n",
            "Epoch: 051 loss_train: 0.0068 acc_train: 1.0000 time: 0.0048s\n",
            "Epoch: 052 loss_train: 0.0110 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 053 loss_train: 0.0054 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 054 loss_train: 0.0149 acc_train: 1.0000 time: 0.0066s\n",
            "Epoch: 055 loss_train: 0.0040 acc_train: 1.0000 time: 0.0048s\n",
            "Epoch: 056 loss_train: 0.0054 acc_train: 1.0000 time: 0.0063s\n",
            "Epoch: 057 loss_train: 0.0037 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 058 loss_train: 0.0044 acc_train: 1.0000 time: 0.0050s\n",
            "Epoch: 059 loss_train: 0.0025 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 060 loss_train: 0.0034 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 061 loss_train: 0.0018 acc_train: 1.0000 time: 0.0058s\n",
            "Epoch: 062 loss_train: 0.0053 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 063 loss_train: 0.0080 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 064 loss_train: 0.0019 acc_train: 1.0000 time: 0.0050s\n",
            "Epoch: 065 loss_train: 0.0036 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 066 loss_train: 0.0023 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 067 loss_train: 0.0016 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 068 loss_train: 0.0040 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 069 loss_train: 0.0009 acc_train: 1.0000 time: 0.0050s\n",
            "Epoch: 070 loss_train: 0.0019 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 071 loss_train: 0.0017 acc_train: 1.0000 time: 0.0048s\n",
            "Epoch: 072 loss_train: 0.0013 acc_train: 1.0000 time: 0.0052s\n",
            "Epoch: 073 loss_train: 0.0115 acc_train: 1.0000 time: 0.0060s\n",
            "Epoch: 074 loss_train: 0.0013 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 075 loss_train: 0.0010 acc_train: 1.0000 time: 0.0044s\n",
            "Epoch: 076 loss_train: 0.0055 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 077 loss_train: 0.0015 acc_train: 1.0000 time: 0.0046s\n",
            "Epoch: 078 loss_train: 0.0014 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 079 loss_train: 0.0048 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 080 loss_train: 0.0023 acc_train: 1.0000 time: 0.0044s\n",
            "Epoch: 081 loss_train: 0.0009 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 082 loss_train: 0.0023 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 083 loss_train: 0.0009 acc_train: 1.0000 time: 0.0046s\n",
            "Epoch: 084 loss_train: 0.0129 acc_train: 1.0000 time: 0.0044s\n",
            "Epoch: 085 loss_train: 0.0018 acc_train: 1.0000 time: 0.0047s\n",
            "Epoch: 086 loss_train: 0.0022 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 087 loss_train: 0.0024 acc_train: 1.0000 time: 0.0049s\n",
            "Epoch: 088 loss_train: 0.0014 acc_train: 1.0000 time: 0.0043s\n",
            "Epoch: 089 loss_train: 0.0004 acc_train: 1.0000 time: 0.0046s\n",
            "Epoch: 090 loss_train: 0.0039 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 091 loss_train: 0.0014 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 092 loss_train: 0.0011 acc_train: 1.0000 time: 0.0045s\n",
            "Epoch: 093 loss_train: 0.0008 acc_train: 1.0000 time: 0.0043s\n",
            "Epoch: 094 loss_train: 0.0061 acc_train: 1.0000 time: 0.0046s\n",
            "Epoch: 095 loss_train: 0.0104 acc_train: 1.0000 time: 0.0052s\n",
            "Epoch: 096 loss_train: 0.0010 acc_train: 1.0000 time: 0.0046s\n",
            "Epoch: 097 loss_train: 0.0007 acc_train: 1.0000 time: 0.0044s\n",
            "Epoch: 098 loss_train: 0.0011 acc_train: 1.0000 time: 0.0070s\n",
            "Epoch: 099 loss_train: 0.0005 acc_train: 1.0000 time: 0.0050s\n",
            "Epoch: 100 loss_train: 0.0005 acc_train: 1.0000 time: 0.0047s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.8993s\n",
            "\n",
            "Test set results: loss= 0.0000 accuracy= 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#After changing features\n",
        "!python '/content/drive/MyDrive/Lab5_Benyahia_Mohamed/code/part2/gnn_karate.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3hsTxIMi0JF",
        "outputId": "5b5492c7-49e4-4d80-ef15-bddd3e0872f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "78\n",
            "/content/drive/MyDrive/ALTEGRAD_lab_5_DLForGraphs_2024/code/part2/utils.py:55: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "Epoch: 001 loss_train: 0.6934 acc_train: 0.4444 time: 1.4959s\n",
            "Epoch: 002 loss_train: 0.6896 acc_train: 0.5556 time: 0.0056s\n",
            "Epoch: 003 loss_train: 0.6874 acc_train: 0.5556 time: 0.0049s\n",
            "Epoch: 004 loss_train: 0.6908 acc_train: 0.5556 time: 0.0043s\n",
            "Epoch: 005 loss_train: 0.6923 acc_train: 0.5556 time: 0.0044s\n",
            "Epoch: 006 loss_train: 0.6897 acc_train: 0.5556 time: 0.0068s\n",
            "Epoch: 007 loss_train: 0.6853 acc_train: 0.5556 time: 0.0044s\n",
            "Epoch: 008 loss_train: 0.6876 acc_train: 0.5556 time: 0.0042s\n",
            "Epoch: 009 loss_train: 0.6865 acc_train: 0.5556 time: 0.0042s\n",
            "Epoch: 010 loss_train: 0.6866 acc_train: 0.5556 time: 0.0049s\n",
            "Epoch: 011 loss_train: 0.6901 acc_train: 0.5556 time: 0.0042s\n",
            "Epoch: 012 loss_train: 0.6807 acc_train: 0.5556 time: 0.0053s\n",
            "Epoch: 013 loss_train: 0.6844 acc_train: 0.5556 time: 0.0043s\n",
            "Epoch: 014 loss_train: 0.6895 acc_train: 0.5556 time: 0.0045s\n",
            "Epoch: 015 loss_train: 0.6898 acc_train: 0.5556 time: 0.0045s\n",
            "Epoch: 016 loss_train: 0.6805 acc_train: 0.5926 time: 0.0060s\n",
            "Epoch: 017 loss_train: 0.6831 acc_train: 0.5556 time: 0.0048s\n",
            "Epoch: 018 loss_train: 0.6804 acc_train: 0.5556 time: 0.0047s\n",
            "Epoch: 019 loss_train: 0.6822 acc_train: 0.5556 time: 0.0045s\n",
            "Epoch: 020 loss_train: 0.6834 acc_train: 0.5556 time: 0.0048s\n",
            "Epoch: 021 loss_train: 0.6719 acc_train: 0.5556 time: 0.0044s\n",
            "Epoch: 022 loss_train: 0.6867 acc_train: 0.5556 time: 0.0042s\n",
            "Epoch: 023 loss_train: 0.6732 acc_train: 0.5556 time: 0.0040s\n",
            "Epoch: 024 loss_train: 0.6668 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 025 loss_train: 0.6797 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 026 loss_train: 0.6698 acc_train: 0.5926 time: 0.0041s\n",
            "Epoch: 027 loss_train: 0.6809 acc_train: 0.5556 time: 0.0041s\n",
            "Epoch: 028 loss_train: 0.6823 acc_train: 0.6296 time: 0.0040s\n",
            "Epoch: 029 loss_train: 0.6681 acc_train: 0.6667 time: 0.0042s\n",
            "Epoch: 030 loss_train: 0.6734 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 031 loss_train: 0.6744 acc_train: 0.6296 time: 0.0040s\n",
            "Epoch: 032 loss_train: 0.6771 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 033 loss_train: 0.6638 acc_train: 0.6296 time: 0.0039s\n",
            "Epoch: 034 loss_train: 0.6937 acc_train: 0.6296 time: 0.0043s\n",
            "Epoch: 035 loss_train: 0.6692 acc_train: 0.5556 time: 0.0042s\n",
            "Epoch: 036 loss_train: 0.6849 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 037 loss_train: 0.6597 acc_train: 0.6296 time: 0.0042s\n",
            "Epoch: 038 loss_train: 0.6668 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 039 loss_train: 0.6742 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 040 loss_train: 0.6745 acc_train: 0.5556 time: 0.0040s\n",
            "Epoch: 041 loss_train: 0.6479 acc_train: 0.6667 time: 0.0041s\n",
            "Epoch: 042 loss_train: 0.6767 acc_train: 0.5926 time: 0.0041s\n",
            "Epoch: 043 loss_train: 0.6674 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 044 loss_train: 0.6769 acc_train: 0.5926 time: 0.0041s\n",
            "Epoch: 045 loss_train: 0.6967 acc_train: 0.6296 time: 0.0040s\n",
            "Epoch: 046 loss_train: 0.6657 acc_train: 0.6296 time: 0.0040s\n",
            "Epoch: 047 loss_train: 0.6375 acc_train: 0.6667 time: 0.0042s\n",
            "Epoch: 048 loss_train: 0.6895 acc_train: 0.6296 time: 0.0042s\n",
            "Epoch: 049 loss_train: 0.6660 acc_train: 0.5926 time: 0.0041s\n",
            "Epoch: 050 loss_train: 0.6654 acc_train: 0.6667 time: 0.0039s\n",
            "Epoch: 051 loss_train: 0.6750 acc_train: 0.6296 time: 0.0038s\n",
            "Epoch: 052 loss_train: 0.6440 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 053 loss_train: 0.6817 acc_train: 0.5926 time: 0.0041s\n",
            "Epoch: 054 loss_train: 0.6794 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 055 loss_train: 0.6877 acc_train: 0.5926 time: 0.0043s\n",
            "Epoch: 056 loss_train: 0.6078 acc_train: 0.7037 time: 0.0039s\n",
            "Epoch: 057 loss_train: 0.6730 acc_train: 0.5926 time: 0.0040s\n",
            "Epoch: 058 loss_train: 0.6589 acc_train: 0.6667 time: 0.0043s\n",
            "Epoch: 059 loss_train: 0.6636 acc_train: 0.6296 time: 0.0048s\n",
            "Epoch: 060 loss_train: 0.6495 acc_train: 0.6667 time: 0.0048s\n",
            "Epoch: 061 loss_train: 0.6980 acc_train: 0.5926 time: 0.0041s\n",
            "Epoch: 062 loss_train: 0.6996 acc_train: 0.5926 time: 0.0040s\n",
            "Epoch: 063 loss_train: 0.6861 acc_train: 0.5926 time: 0.0040s\n",
            "Epoch: 064 loss_train: 0.6632 acc_train: 0.6296 time: 0.0040s\n",
            "Epoch: 065 loss_train: 0.6603 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 066 loss_train: 0.6338 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 067 loss_train: 0.6537 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 068 loss_train: 0.6456 acc_train: 0.7037 time: 0.0041s\n",
            "Epoch: 069 loss_train: 0.6745 acc_train: 0.5926 time: 0.0041s\n",
            "Epoch: 070 loss_train: 0.6648 acc_train: 0.6296 time: 0.0039s\n",
            "Epoch: 071 loss_train: 0.6634 acc_train: 0.6667 time: 0.0041s\n",
            "Epoch: 072 loss_train: 0.6566 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 073 loss_train: 0.6742 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 074 loss_train: 0.6676 acc_train: 0.5926 time: 0.0062s\n",
            "Epoch: 075 loss_train: 0.6788 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 076 loss_train: 0.6564 acc_train: 0.6667 time: 0.0041s\n",
            "Epoch: 077 loss_train: 0.6293 acc_train: 0.6667 time: 0.0040s\n",
            "Epoch: 078 loss_train: 0.6509 acc_train: 0.7037 time: 0.0041s\n",
            "Epoch: 079 loss_train: 0.6328 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 080 loss_train: 0.6278 acc_train: 0.6667 time: 0.0042s\n",
            "Epoch: 081 loss_train: 0.6446 acc_train: 0.6667 time: 0.0041s\n",
            "Epoch: 082 loss_train: 0.6556 acc_train: 0.7037 time: 0.0041s\n",
            "Epoch: 083 loss_train: 0.6690 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 084 loss_train: 0.6062 acc_train: 0.6667 time: 0.0039s\n",
            "Epoch: 085 loss_train: 0.6709 acc_train: 0.5926 time: 0.0042s\n",
            "Epoch: 086 loss_train: 0.6620 acc_train: 0.5926 time: 0.0041s\n",
            "Epoch: 087 loss_train: 0.7145 acc_train: 0.5556 time: 0.0039s\n",
            "Epoch: 088 loss_train: 0.6571 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 089 loss_train: 0.6281 acc_train: 0.6667 time: 0.0038s\n",
            "Epoch: 090 loss_train: 0.6717 acc_train: 0.6296 time: 0.0041s\n",
            "Epoch: 091 loss_train: 0.6274 acc_train: 0.6667 time: 0.0042s\n",
            "Epoch: 092 loss_train: 0.6648 acc_train: 0.6296 time: 0.0039s\n",
            "Epoch: 093 loss_train: 0.6837 acc_train: 0.6296 time: 0.0042s\n",
            "Epoch: 094 loss_train: 0.6924 acc_train: 0.6296 time: 0.0038s\n",
            "Epoch: 095 loss_train: 0.6957 acc_train: 0.6296 time: 0.0075s\n",
            "Epoch: 096 loss_train: 0.6641 acc_train: 0.5926 time: 0.0044s\n",
            "Epoch: 097 loss_train: 0.6483 acc_train: 0.6296 time: 0.0044s\n",
            "Epoch: 098 loss_train: 0.6683 acc_train: 0.6296 time: 0.0044s\n",
            "Epoch: 099 loss_train: 0.6451 acc_train: 0.5926 time: 0.0046s\n",
            "Epoch: 100 loss_train: 0.6375 acc_train: 0.6667 time: 0.0043s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.9283s\n",
            "\n",
            "Test set results: loss= 0.8210 accuracy= 0.4286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python '/content/drive/MyDrive/Lab5_Benyahia_Mohamed/code/part2/gnn_cora.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEt4iuvdS1T9",
        "outputId": "4d476566-34cb-4685-e922-9a329785f77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has 2708 nodes, 5429 edges, 1433 features.\n",
            "/content/drive/MyDrive/ALTEGRAD_lab_5_DLForGraphs_2024/code/part2/utils.py:55: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n",
            "Epoch: 001 loss_train: 1.9745 acc_train: 0.1576 loss_val: 1.9560 acc_val: 0.1697 time: 0.8321s\n",
            "Epoch: 002 loss_train: 1.9576 acc_train: 0.1576 loss_val: 1.9414 acc_val: 0.1753 time: 0.0112s\n",
            "Epoch: 003 loss_train: 1.9411 acc_train: 0.1601 loss_val: 1.9251 acc_val: 0.1956 time: 0.0110s\n",
            "Epoch: 004 loss_train: 1.9227 acc_train: 0.2248 loss_val: 1.9065 acc_val: 0.1697 time: 0.0110s\n",
            "Epoch: 005 loss_train: 1.9020 acc_train: 0.2272 loss_val: 1.8853 acc_val: 0.2970 time: 0.0122s\n",
            "Epoch: 006 loss_train: 1.8772 acc_train: 0.2968 loss_val: 1.8627 acc_val: 0.2970 time: 0.0121s\n",
            "Epoch: 007 loss_train: 1.8522 acc_train: 0.2968 loss_val: 1.8416 acc_val: 0.2970 time: 0.0106s\n",
            "Epoch: 008 loss_train: 1.8268 acc_train: 0.2968 loss_val: 1.8291 acc_val: 0.2970 time: 0.0106s\n",
            "Epoch: 009 loss_train: 1.8084 acc_train: 0.2968 loss_val: 1.8311 acc_val: 0.2970 time: 0.0108s\n",
            "Epoch: 010 loss_train: 1.8008 acc_train: 0.2968 loss_val: 1.8364 acc_val: 0.2970 time: 0.0107s\n",
            "Epoch: 011 loss_train: 1.8045 acc_train: 0.2968 loss_val: 1.8272 acc_val: 0.2970 time: 0.0106s\n",
            "Epoch: 012 loss_train: 1.7939 acc_train: 0.2968 loss_val: 1.8065 acc_val: 0.2970 time: 0.0108s\n",
            "Epoch: 013 loss_train: 1.7728 acc_train: 0.2968 loss_val: 1.7821 acc_val: 0.2970 time: 0.0106s\n",
            "Epoch: 014 loss_train: 1.7530 acc_train: 0.2968 loss_val: 1.7593 acc_val: 0.2970 time: 0.0105s\n",
            "Epoch: 015 loss_train: 1.7343 acc_train: 0.2980 loss_val: 1.7385 acc_val: 0.2989 time: 0.0107s\n",
            "Epoch: 016 loss_train: 1.7133 acc_train: 0.2980 loss_val: 1.7177 acc_val: 0.3026 time: 0.0107s\n",
            "Epoch: 017 loss_train: 1.6923 acc_train: 0.2999 loss_val: 1.6955 acc_val: 0.3155 time: 0.0107s\n",
            "Epoch: 018 loss_train: 1.6730 acc_train: 0.3183 loss_val: 1.6705 acc_val: 0.3487 time: 0.0106s\n",
            "Epoch: 019 loss_train: 1.6488 acc_train: 0.3436 loss_val: 1.6423 acc_val: 0.3635 time: 0.0107s\n",
            "Epoch: 020 loss_train: 1.6217 acc_train: 0.3633 loss_val: 1.6108 acc_val: 0.3819 time: 0.0108s\n",
            "Epoch: 021 loss_train: 1.5885 acc_train: 0.3682 loss_val: 1.5765 acc_val: 0.3893 time: 0.0126s\n",
            "Epoch: 022 loss_train: 1.5520 acc_train: 0.3781 loss_val: 1.5404 acc_val: 0.3893 time: 0.0100s\n",
            "Epoch: 023 loss_train: 1.5135 acc_train: 0.3799 loss_val: 1.5032 acc_val: 0.3893 time: 0.0094s\n",
            "Epoch: 024 loss_train: 1.4750 acc_train: 0.3842 loss_val: 1.4653 acc_val: 0.3893 time: 0.0093s\n",
            "Epoch: 025 loss_train: 1.4367 acc_train: 0.3922 loss_val: 1.4263 acc_val: 0.3911 time: 0.0096s\n",
            "Epoch: 026 loss_train: 1.3946 acc_train: 0.3947 loss_val: 1.3863 acc_val: 0.4059 time: 0.0094s\n",
            "Epoch: 027 loss_train: 1.3558 acc_train: 0.4095 loss_val: 1.3459 acc_val: 0.4244 time: 0.0093s\n",
            "Epoch: 028 loss_train: 1.3139 acc_train: 0.4212 loss_val: 1.3063 acc_val: 0.4576 time: 0.0092s\n",
            "Epoch: 029 loss_train: 1.2731 acc_train: 0.4550 loss_val: 1.2672 acc_val: 0.5295 time: 0.0096s\n",
            "Epoch: 030 loss_train: 1.2374 acc_train: 0.5259 loss_val: 1.2290 acc_val: 0.5793 time: 0.0093s\n",
            "Epoch: 031 loss_train: 1.1982 acc_train: 0.5770 loss_val: 1.1918 acc_val: 0.6015 time: 0.0094s\n",
            "Epoch: 032 loss_train: 1.1555 acc_train: 0.6133 loss_val: 1.1562 acc_val: 0.6125 time: 0.0094s\n",
            "Epoch: 033 loss_train: 1.1217 acc_train: 0.6250 loss_val: 1.1227 acc_val: 0.6273 time: 0.0095s\n",
            "Epoch: 034 loss_train: 1.0938 acc_train: 0.6330 loss_val: 1.0909 acc_val: 0.6328 time: 0.0093s\n",
            "Epoch: 035 loss_train: 1.0527 acc_train: 0.6589 loss_val: 1.0588 acc_val: 0.6347 time: 0.0094s\n",
            "Epoch: 036 loss_train: 1.0111 acc_train: 0.6700 loss_val: 1.0271 acc_val: 0.6421 time: 0.0093s\n",
            "Epoch: 037 loss_train: 0.9771 acc_train: 0.6755 loss_val: 0.9971 acc_val: 0.6494 time: 0.0092s\n",
            "Epoch: 038 loss_train: 0.9425 acc_train: 0.6804 loss_val: 0.9695 acc_val: 0.6513 time: 0.0093s\n",
            "Epoch: 039 loss_train: 0.9156 acc_train: 0.6804 loss_val: 0.9434 acc_val: 0.6550 time: 0.0093s\n",
            "Epoch: 040 loss_train: 0.8878 acc_train: 0.6946 loss_val: 0.9191 acc_val: 0.6587 time: 0.0092s\n",
            "Epoch: 041 loss_train: 0.8573 acc_train: 0.7063 loss_val: 0.8951 acc_val: 0.6716 time: 0.0094s\n",
            "Epoch: 042 loss_train: 0.8228 acc_train: 0.7167 loss_val: 0.8718 acc_val: 0.6863 time: 0.0094s\n",
            "Epoch: 043 loss_train: 0.7959 acc_train: 0.7303 loss_val: 0.8488 acc_val: 0.6993 time: 0.0090s\n",
            "Epoch: 044 loss_train: 0.7686 acc_train: 0.7414 loss_val: 0.8264 acc_val: 0.7066 time: 0.0088s\n",
            "Epoch: 045 loss_train: 0.7405 acc_train: 0.7512 loss_val: 0.8045 acc_val: 0.7159 time: 0.0104s\n",
            "Epoch: 046 loss_train: 0.7210 acc_train: 0.7635 loss_val: 0.7832 acc_val: 0.7325 time: 0.0089s\n",
            "Epoch: 047 loss_train: 0.6835 acc_train: 0.7734 loss_val: 0.7629 acc_val: 0.7454 time: 0.0089s\n",
            "Epoch: 048 loss_train: 0.6707 acc_train: 0.7869 loss_val: 0.7431 acc_val: 0.7601 time: 0.0090s\n",
            "Epoch: 049 loss_train: 0.6436 acc_train: 0.7906 loss_val: 0.7232 acc_val: 0.7768 time: 0.0091s\n",
            "Epoch: 050 loss_train: 0.6207 acc_train: 0.8103 loss_val: 0.7035 acc_val: 0.7841 time: 0.0087s\n",
            "Epoch: 051 loss_train: 0.5923 acc_train: 0.8122 loss_val: 0.6838 acc_val: 0.7878 time: 0.0088s\n",
            "Epoch: 052 loss_train: 0.5804 acc_train: 0.8165 loss_val: 0.6646 acc_val: 0.7952 time: 0.0090s\n",
            "Epoch: 053 loss_train: 0.5597 acc_train: 0.8165 loss_val: 0.6459 acc_val: 0.8007 time: 0.0099s\n",
            "Epoch: 054 loss_train: 0.5404 acc_train: 0.8220 loss_val: 0.6283 acc_val: 0.8118 time: 0.0094s\n",
            "Epoch: 055 loss_train: 0.5231 acc_train: 0.8282 loss_val: 0.6104 acc_val: 0.8155 time: 0.0088s\n",
            "Epoch: 056 loss_train: 0.4914 acc_train: 0.8467 loss_val: 0.5927 acc_val: 0.8192 time: 0.0087s\n",
            "Epoch: 057 loss_train: 0.4890 acc_train: 0.8448 loss_val: 0.5758 acc_val: 0.8247 time: 0.0088s\n",
            "Epoch: 058 loss_train: 0.4538 acc_train: 0.8651 loss_val: 0.5600 acc_val: 0.8339 time: 0.0088s\n",
            "Epoch: 059 loss_train: 0.4392 acc_train: 0.8762 loss_val: 0.5446 acc_val: 0.8432 time: 0.0089s\n",
            "Epoch: 060 loss_train: 0.4194 acc_train: 0.8892 loss_val: 0.5303 acc_val: 0.8561 time: 0.0087s\n",
            "Epoch: 061 loss_train: 0.4060 acc_train: 0.8922 loss_val: 0.5179 acc_val: 0.8579 time: 0.0093s\n",
            "Epoch: 062 loss_train: 0.3792 acc_train: 0.9107 loss_val: 0.5064 acc_val: 0.8616 time: 0.0091s\n",
            "Epoch: 063 loss_train: 0.3638 acc_train: 0.9150 loss_val: 0.4970 acc_val: 0.8690 time: 0.0089s\n",
            "Epoch: 064 loss_train: 0.3584 acc_train: 0.9039 loss_val: 0.4911 acc_val: 0.8708 time: 0.0093s\n",
            "Epoch: 065 loss_train: 0.3450 acc_train: 0.9230 loss_val: 0.4860 acc_val: 0.8727 time: 0.0091s\n",
            "Epoch: 066 loss_train: 0.3278 acc_train: 0.9193 loss_val: 0.4809 acc_val: 0.8690 time: 0.0088s\n",
            "Epoch: 067 loss_train: 0.3163 acc_train: 0.9187 loss_val: 0.4753 acc_val: 0.8672 time: 0.0089s\n",
            "Epoch: 068 loss_train: 0.3032 acc_train: 0.9193 loss_val: 0.4670 acc_val: 0.8690 time: 0.0089s\n",
            "Epoch: 069 loss_train: 0.2944 acc_train: 0.9200 loss_val: 0.4593 acc_val: 0.8708 time: 0.0093s\n",
            "Epoch: 070 loss_train: 0.2810 acc_train: 0.9236 loss_val: 0.4556 acc_val: 0.8690 time: 0.0091s\n",
            "Epoch: 071 loss_train: 0.2721 acc_train: 0.9267 loss_val: 0.4533 acc_val: 0.8672 time: 0.0099s\n",
            "Epoch: 072 loss_train: 0.2611 acc_train: 0.9292 loss_val: 0.4512 acc_val: 0.8690 time: 0.0160s\n",
            "Epoch: 073 loss_train: 0.2462 acc_train: 0.9286 loss_val: 0.4516 acc_val: 0.8708 time: 0.0087s\n",
            "Epoch: 074 loss_train: 0.2367 acc_train: 0.9347 loss_val: 0.4521 acc_val: 0.8708 time: 0.0089s\n",
            "Epoch: 075 loss_train: 0.2284 acc_train: 0.9317 loss_val: 0.4540 acc_val: 0.8690 time: 0.0087s\n",
            "Epoch: 076 loss_train: 0.2312 acc_train: 0.9341 loss_val: 0.4540 acc_val: 0.8653 time: 0.0089s\n",
            "Epoch: 077 loss_train: 0.2236 acc_train: 0.9310 loss_val: 0.4532 acc_val: 0.8708 time: 0.0089s\n",
            "Epoch: 078 loss_train: 0.2202 acc_train: 0.9366 loss_val: 0.4508 acc_val: 0.8727 time: 0.0095s\n",
            "Epoch: 079 loss_train: 0.2142 acc_train: 0.9323 loss_val: 0.4481 acc_val: 0.8727 time: 0.0092s\n",
            "Epoch: 080 loss_train: 0.2073 acc_train: 0.9384 loss_val: 0.4470 acc_val: 0.8708 time: 0.0087s\n",
            "Epoch: 081 loss_train: 0.1929 acc_train: 0.9397 loss_val: 0.4489 acc_val: 0.8727 time: 0.0088s\n",
            "Epoch: 082 loss_train: 0.1960 acc_train: 0.9403 loss_val: 0.4507 acc_val: 0.8727 time: 0.0087s\n",
            "Epoch: 083 loss_train: 0.1853 acc_train: 0.9446 loss_val: 0.4522 acc_val: 0.8764 time: 0.0087s\n",
            "Epoch: 084 loss_train: 0.1859 acc_train: 0.9390 loss_val: 0.4541 acc_val: 0.8782 time: 0.0087s\n",
            "Epoch: 085 loss_train: 0.1827 acc_train: 0.9378 loss_val: 0.4535 acc_val: 0.8764 time: 0.0087s\n",
            "Epoch: 086 loss_train: 0.1768 acc_train: 0.9464 loss_val: 0.4523 acc_val: 0.8801 time: 0.0108s\n",
            "Epoch: 087 loss_train: 0.1755 acc_train: 0.9421 loss_val: 0.4520 acc_val: 0.8782 time: 0.0088s\n",
            "Epoch: 088 loss_train: 0.1694 acc_train: 0.9483 loss_val: 0.4513 acc_val: 0.8801 time: 0.0087s\n",
            "Epoch: 089 loss_train: 0.1647 acc_train: 0.9495 loss_val: 0.4524 acc_val: 0.8801 time: 0.0088s\n",
            "Epoch: 090 loss_train: 0.1628 acc_train: 0.9421 loss_val: 0.4539 acc_val: 0.8782 time: 0.0088s\n",
            "Epoch: 091 loss_train: 0.1573 acc_train: 0.9470 loss_val: 0.4572 acc_val: 0.8801 time: 0.0088s\n",
            "Epoch: 092 loss_train: 0.1581 acc_train: 0.9440 loss_val: 0.4617 acc_val: 0.8801 time: 0.0088s\n",
            "Epoch: 093 loss_train: 0.1517 acc_train: 0.9501 loss_val: 0.4679 acc_val: 0.8838 time: 0.0089s\n",
            "Epoch: 094 loss_train: 0.1524 acc_train: 0.9507 loss_val: 0.4702 acc_val: 0.8782 time: 0.0088s\n",
            "Epoch: 095 loss_train: 0.1459 acc_train: 0.9514 loss_val: 0.4728 acc_val: 0.8838 time: 0.0087s\n",
            "Epoch: 096 loss_train: 0.1452 acc_train: 0.9526 loss_val: 0.4748 acc_val: 0.8819 time: 0.0088s\n",
            "Epoch: 097 loss_train: 0.1433 acc_train: 0.9569 loss_val: 0.4774 acc_val: 0.8819 time: 0.0089s\n",
            "Epoch: 098 loss_train: 0.1335 acc_train: 0.9532 loss_val: 0.4811 acc_val: 0.8801 time: 0.0088s\n",
            "Epoch: 099 loss_train: 0.1331 acc_train: 0.9581 loss_val: 0.4849 acc_val: 0.8801 time: 0.0088s\n",
            "Epoch: 100 loss_train: 0.1344 acc_train: 0.9514 loss_val: 0.4897 acc_val: 0.8782 time: 0.0087s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.7840s\n",
            "\n",
            "Test set results: loss= 0.5456 accuracy= 0.8598\n",
            "Figure(1500x900)\n"
          ]
        }
      ]
    }
  ]
}